# LLM Bench

This repository is a proof-of-concept benchmark for several Language Model
Models (LLMs) using Langchain and Streamlit.

## Introduction

LLMs have gained significant attention in the field of natural language
processing (NLP) due to their ability to generate human-like text. However,
comparing the performance of different LLMs can be challenging. This repository
aims to address this challenge by providing a benchmarking framework using
Langchain and Streamlit.

## Features

- Benchmark multiple LLMs: The repository allows you to benchmark multiple LLMs and compare their performance on various NLP tasks.
- Streamlit interface: The benchmarking results are presented in an interactive and user-friendly Streamlit interface, making it easy to analyze and compare the performance of different LLMs.
- Langchain integration: Langchain is used as the underlying framework to handle the benchmarking process, providing a seamless and efficient experience.

## Getting Started

To get started with benchmarking LLMs using this repository, follow these steps:

1. Clone the repository: `git clone https://github.com/your-username/llm-bench.git`
2. Install the required dependencies: `pip install -r requirements.txt`
3. Configure the LLMs: Edit the configuration file to specify the LLMs you want to benchmark and their respective settings.
4. Run the benchmark: Execute the benchmarking script to start the benchmarking process.
5. View the results: Open the Streamlit interface in your browser to view and analyze the benchmarking results.
